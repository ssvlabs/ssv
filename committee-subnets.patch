diff --git a/cli/operator/node.go b/cli/operator/node.go
index 1bee482ab..ceb1fde7a 100644
--- a/cli/operator/node.go
+++ b/cli/operator/node.go
@@ -236,7 +236,7 @@ var StartNodeCmd = &cobra.Command{
 		cfg.SSVOptions.ExecutionClient = executionClient
 		cfg.SSVOptions.Network = networkConfig
 		cfg.SSVOptions.P2PNetwork = p2pNetwork
-		cfg.SSVOptions.ValidatorOptions.BeaconNetwork = networkConfig.Beacon.GetNetwork()
+		cfg.SSVOptions.ValidatorOptions.NetworkConfig = networkConfig
 		cfg.SSVOptions.ValidatorOptions.Context = cmd.Context()
 		cfg.SSVOptions.ValidatorOptions.DB = db
 		cfg.SSVOptions.ValidatorOptions.Network = p2pNetwork
@@ -322,7 +322,7 @@ var StartNodeCmd = &cobra.Command{
 		)
 		nodeProber.AddNode("event syncer", eventSyncer)
 
-		cfg.P2pNetworkConfig.GetValidatorStats = func() (uint64, uint64, uint64, error) {
+		cfg.P2pNetworkConfig.GetValidatorStats = func() (network.ValidatorStats, error) {
 			return validatorCtrl.GetValidatorStats()
 		}
 		if err := p2pNetwork.Setup(logger); err != nil {
diff --git a/cluster-subnets-todo.md b/cluster-subnets-todo.md
new file mode 100644
index 000000000..8dfed47fa
--- /dev/null
+++ b/cluster-subnets-todo.md
@@ -0,0 +1,16 @@
+### TODO
+
+- [ ] UpdateScoreParams every slot or so validators were added.
+- [ ] Post-fork: don't connect nodes who didn't upgrade
+- [ ] Fix broken tests, ideally also add committee subnets to existing tests
+
+### DONE
+
+- [x] Pre-subscribe to committee subnets before fork epoch
+- [x] Swap topic score params on fork epoch
+- [x] p2p subscribe 2 slots before fork
+- [x] p2p unsubscribe on fork epoch
+- [x] discovery superimposed advertisement 8 slots before fork (to give peers enough time to make the right connections)
+- [x] discovery committee subnets only advertisement on fork epoch
+- [x] unregister old message validator & register new
+- [x] see TODO in network/topics/controller.go
diff --git a/message/validation/validation.go b/message/validation/validation.go
index fc83ef4f8..4fa6dfc3f 100644
--- a/message/validation/validation.go
+++ b/message/validation/validation.go
@@ -370,10 +370,19 @@ func (mv *messageValidator) validateP2PMessage(pMsg *pubsub.Message, receivedAt
 	// Check if the message was sent on the right topic.
 	currentTopic := pMsg.GetTopic()
 	currentTopicBaseName := commons.GetTopicBaseName(currentTopic)
-	topics := commons.ValidatorTopicID(msg.GetID().GetPubKey())
+	var expectedTopics []string
+	if mv.netCfg.CommitteeSubnetsFork() {
+		share := mv.nodeStorage.Shares().Get(nil, msg.GetID().GetPubKey())
+		if share == nil {
+			return nil, Descriptor{}, ErrUnknownValidator
+		}
+		expectedTopics = commons.CommitteeTopicID(share.CommitteeID())
+	} else {
+		expectedTopics = commons.ValidatorTopicID(msg.GetID().GetPubKey())
+	}
 
 	topicFound := false
-	for _, tp := range topics {
+	for _, tp := range expectedTopics {
 		if tp == currentTopicBaseName {
 			topicFound = true
 			break
diff --git a/network/commons/common.go b/network/commons/common.go
index 2cfd33edd..0cac95068 100644
--- a/network/commons/common.go
+++ b/network/commons/common.go
@@ -4,6 +4,7 @@ import (
 	"encoding/binary"
 	"encoding/hex"
 	"fmt"
+	"math/big"
 	"strconv"
 	"strings"
 	"time"
@@ -14,6 +15,7 @@ import (
 	spectypes "github.com/ssvlabs/ssv-spec/types"
 
 	p2pprotocol "github.com/ssvlabs/ssv/protocol/v2/p2p"
+	"github.com/ssvlabs/ssv/protocol/v2/types"
 )
 
 const (
@@ -22,8 +24,8 @@ const (
 
 	peersForSync = 10
 
-	// subnetsCount returns the subnet count for genesis
-	subnetsCount uint64 = 128
+	// SubnetsCount returns the subnet count for genesis
+	SubnetsCount uint64 = 128
 
 	// UnknownSubnet is used when a validator public key is invalid
 	UnknownSubnet = "unknown"
@@ -54,6 +56,11 @@ func ValidatorTopicID(pkByts []byte) []string {
 	return []string{SubnetTopicID(subnet)}
 }
 
+// CommitteeTopicID returns the topic to use for the given committee
+func CommitteeTopicID(cid types.CommitteeID) []string {
+	return []string{strconv.Itoa(CommitteeSubnet(cid))}
+}
+
 // GetTopicFullName returns the topic full name, including prefix
 func GetTopicFullName(baseName string) string {
 	return fmt.Sprintf("%s.%s", topicPrefix, baseName)
@@ -70,7 +77,13 @@ func ValidatorSubnet(validatorPKHex string) int {
 		return -1
 	}
 	val := hexToUint64(validatorPKHex[:10])
-	return int(val % subnetsCount)
+	return int(val % SubnetsCount)
+}
+
+// CommitteeSubnet returns the subnet for the given committee
+func CommitteeSubnet(cid types.CommitteeID) int {
+	subnet := new(big.Int).Mod(new(big.Int).SetBytes(cid[:]), new(big.Int).SetUint64(SubnetsCount))
+	return int(subnet.Int64())
 }
 
 // MsgIDFunc is the function that maps a message to a msg_id
@@ -91,7 +104,7 @@ func MsgID() MsgIDFunc {
 
 // Subnets returns the subnets count for this fork
 func Subnets() int {
-	return int(subnetsCount)
+	return int(SubnetsCount)
 }
 
 // Topics returns the available topics for this fork.
diff --git a/network/network.go b/network/network.go
index e9b94b9bc..886d1efcb 100644
--- a/network/network.go
+++ b/network/network.go
@@ -6,6 +6,7 @@ import (
 
 	"go.uber.org/zap"
 
+	"github.com/ssvlabs/ssv/network/commons"
 	protocolp2p "github.com/ssvlabs/ssv/protocol/v2/p2p"
 	"github.com/ssvlabs/ssv/protocol/v2/ssv/queue"
 )
@@ -39,8 +40,16 @@ type P2PNetwork interface {
 	SubscribeRandoms(logger *zap.Logger, numSubnets int) error
 }
 
-// GetValidatorStats returns stats of validators, including the following:
-//   - the amount of validators in the network
-//   - the amount of active validators in the network (i.e. not slashed or existed)
-//   - the amount of validators assigned to this operator
-type GetValidatorStats func() (uint64, uint64, uint64, error)
+type ValidatorCounts struct {
+	Total     uint32
+	Attesting uint32
+	Mine      uint32
+}
+
+type ValidatorStats struct {
+	ValidatorCounts
+	Subnets [commons.SubnetsCount]ValidatorCounts
+}
+
+// GetValidatorStats returns stats of committees & validators.
+type GetValidatorStats func() (ValidatorStats, error)
diff --git a/network/p2p/p2p.go b/network/p2p/p2p.go
index 55d0989d8..83b940155 100644
--- a/network/p2p/p2p.go
+++ b/network/p2p/p2p.go
@@ -2,6 +2,7 @@ package p2pv1
 
 import (
 	"context"
+	"errors"
 	"sync/atomic"
 	"time"
 
@@ -27,6 +28,7 @@ import (
 	operatordatastore "github.com/ssvlabs/ssv/operator/datastore"
 	"github.com/ssvlabs/ssv/operator/keys"
 	operatorstorage "github.com/ssvlabs/ssv/operator/storage"
+	"github.com/ssvlabs/ssv/protocol/v2/types"
 	"github.com/ssvlabs/ssv/utils/async"
 	"github.com/ssvlabs/ssv/utils/tasks"
 )
@@ -71,11 +73,18 @@ type p2pNetwork struct {
 	state int32
 
 	activeValidators *hashmap.Map[string, validatorStatus]
+	activeCommittees *hashmap.Map[string, validatorStatus]
 
 	backoffConnector *libp2pdiscbackoff.BackoffConnector
-	subnets          []byte
 	libConnManager   connmgrcore.ConnManager
 
+	// fixedSubnets are the subnets that the node will not unsubscribe from.
+	fixedSubnets []byte
+
+	// activeSubnets are the subnets that the node is currently subscribed to.
+	// Changes according to the active validators, which is populated by the Subscribe method.
+	activeSubnets []byte
+
 	nodeStorage             operatorstorage.Storage
 	operatorPKHashToPKCache *hashmap.Map[string, []byte] // used for metrics
 	operatorSigner          keys.OperatorSigner
@@ -98,6 +107,7 @@ func New(logger *zap.Logger, cfg *Config, mr Metrics) network.P2PNetwork {
 		msgValidator:            cfg.MessageValidator,
 		state:                   stateClosed,
 		activeValidators:        hashmap.New[string, validatorStatus](),
+		activeCommittees:        hashmap.New[string, validatorStatus](),
 		nodeStorage:             cfg.NodeStorage,
 		operatorPKHashToPKCache: hashmap.New[string, []byte](),
 		operatorSigner:          cfg.OperatorSigner,
@@ -197,7 +207,7 @@ func (n *p2pNetwork) peersBalancing(logger *zap.Logger) func() {
 		defer cancel()
 
 		connMgr := peers.NewConnManager(logger, n.libConnManager, n.idx)
-		mySubnets := records.Subnets(n.subnets).Clone()
+		mySubnets := records.Subnets(n.activeSubnets).Clone()
 		connMgr.TagBestPeers(logger, n.cfg.MaxPeers-1, mySubnets, allPeers, n.cfg.TopicMaxPeers)
 		connMgr.TrimPeers(ctx, logger, n.host.Network())
 	}
@@ -248,45 +258,85 @@ func (n *p2pNetwork) UpdateSubnets(logger *zap.Logger) {
 	for ; true; <-ticker.C {
 		start := time.Now()
 
-		// Compute the new subnets according to the active validators.
-		newSubnets := make([]byte, commons.Subnets())
-		copy(newSubnets, n.subnets)
-		n.activeValidators.Range(func(pkHex string, status validatorStatus) bool {
-			subnet := commons.ValidatorSubnet(pkHex)
-			newSubnets[subnet] = byte(1)
+		// Compute the new subnets according to the active committees/validators.
+		updatedSubnets := make([]byte, commons.Subnets())
+		copy(updatedSubnets, n.fixedSubnets)
+		n.activeCommittees.Range(func(cid string, status validatorStatus) bool {
+			subnet := commons.CommitteeSubnet(types.CommitteeID([]byte(cid)))
+			updatedSubnets[subnet] = byte(1)
 			return true
 		})
-		n.subnets = newSubnets
+		if n.validatorSubnetSubscriptions() { // Pre-fork.
+			n.activeValidators.Range(func(pkHex string, status validatorStatus) bool {
+				subnet := commons.ValidatorSubnet(pkHex)
+				updatedSubnets[subnet] = byte(1)
+				return true
+			})
+		}
+		n.activeSubnets = updatedSubnets
 
 		// Compute the not yet registered subnets.
-		unregisteredSubnets := make([]int, 0)
-		for subnet, active := range newSubnets {
+		addedSubnets := make([]int, 0)
+		for subnet, active := range updatedSubnets {
 			if active == byte(1) && registeredSubnets[subnet] == byte(0) {
-				unregisteredSubnets = append(unregisteredSubnets, subnet)
+				addedSubnets = append(addedSubnets, subnet)
+			}
+		}
+
+		// Compute the not anymore registered subnets.
+		removedSubnets := make([]int, 0)
+		for subnet, active := range registeredSubnets {
+			if active == byte(1) && updatedSubnets[subnet] == byte(0) {
+				removedSubnets = append(removedSubnets, subnet)
 			}
 		}
-		registeredSubnets = newSubnets
 
-		if len(unregisteredSubnets) == 0 {
+		registeredSubnets = updatedSubnets
+
+		if len(addedSubnets) == 0 && len(removedSubnets) == 0 {
 			continue
 		}
 
 		self := n.idx.Self()
-		self.Metadata.Subnets = records.Subnets(n.subnets).String()
+		self.Metadata.Subnets = records.Subnets(n.activeSubnets).String()
 		n.idx.UpdateSelfRecord(self)
 
-		err := n.disc.RegisterSubnets(logger.Named(logging.NameDiscoveryService), unregisteredSubnets...)
-		if err != nil {
-			logger.Warn("could not register subnets", zap.Error(err))
-			continue
+		var errs error
+		if len(addedSubnets) > 0 {
+			err := n.disc.RegisterSubnets(logger.Named(logging.NameDiscoveryService), addedSubnets...)
+			if err != nil {
+				logger.Debug("could not register subnets", zap.Error(err))
+				errs = errors.Join(errs, err)
+			}
 		}
+		if len(removedSubnets) > 0 {
+			err := n.disc.DeregisterSubnets(logger.Named(logging.NameDiscoveryService), removedSubnets...)
+			if err != nil {
+				logger.Debug("could not unregister subnets", zap.Error(err))
+				errs = errors.Join(errs, err)
+			}
+
+			// Unsubscribe from the removed subnets.
+			for _, subnet := range removedSubnets {
+				if err := n.unsubscribeSubnet(logger, uint(subnet)); err != nil {
+					logger.Debug("could not unsubscribe from subnet", zap.Int("subnet", subnet), zap.Error(err))
+					errs = errors.Join(errs, err)
+				} else {
+					logger.Debug("unsubscribed from subnet", zap.Int("subnet", subnet))
+				}
+			}
+		}
+
 		allSubs, _ := records.Subnets{}.FromString(records.AllSubnets)
-		subnetsList := records.SharedSubnets(allSubs, n.subnets, 0)
+		subnetsList := records.SharedSubnets(allSubs, n.activeSubnets, 0)
 		logger.Debug("updated subnets",
-			zap.Any("added", unregisteredSubnets),
+			zap.Any("added", addedSubnets),
+			zap.Any("removed", removedSubnets),
 			zap.Any("subnets", subnetsList),
+			zap.Any("subscribed_topics", n.topicsCtrl.Topics()),
 			zap.Int("total_subnets", len(subnetsList)),
 			zap.Duration("took", time.Since(start)),
+			zap.Error(errs),
 		)
 	}
 }
diff --git a/network/p2p/p2p_forks.go b/network/p2p/p2p_forks.go
new file mode 100644
index 000000000..0866cd256
--- /dev/null
+++ b/network/p2p/p2p_forks.go
@@ -0,0 +1,7 @@
+package p2pv1
+
+// validatorSubnetSubscriptions returns whether the node should subscribe to validator subnets,
+// which is true before the committee subnets fork.
+func (n *p2pNetwork) validatorSubnetSubscriptions() bool {
+	return !n.cfg.Network.CommitteeSubnetsFork()
+}
diff --git a/network/p2p/p2p_pubsub.go b/network/p2p/p2p_pubsub.go
index 04368cd78..f5912e0ff 100644
--- a/network/p2p/p2p_pubsub.go
+++ b/network/p2p/p2p_pubsub.go
@@ -8,6 +8,7 @@ import (
 	"time"
 
 	"github.com/ssvlabs/ssv/protocol/v2/message"
+	"github.com/ssvlabs/ssv/protocol/v2/types"
 
 	pubsub "github.com/libp2p/go-libp2p-pubsub"
 	"github.com/libp2p/go-libp2p/core/peer"
@@ -39,7 +40,10 @@ func (n *p2pNetwork) UseMessageRouter(router network.MessageRouter) {
 // Peers registers a message router to handle incoming messages
 func (n *p2pNetwork) Peers(pk spectypes.ValidatorPK) ([]peer.ID, error) {
 	all := make([]peer.ID, 0)
-	topics := commons.ValidatorTopicID(pk)
+	topics, err := n.validatorTopics(pk)
+	if err != nil {
+		return nil, fmt.Errorf("could not get validator topics: %w", err)
+	}
 	for _, topic := range topics {
 		peers, err := n.topicsCtrl.Peers(topic)
 		if err != nil {
@@ -65,12 +69,13 @@ func (n *p2pNetwork) Broadcast(msgID spectypes.MessageID, msg *spectypes.SignedS
 		return fmt.Errorf("could not encode signed ssv message: %w", err)
 	}
 
-	vpk := msgID.GetPubKey()
-	topics := commons.ValidatorTopicID(vpk)
-
+	topics, err := n.validatorTopics(msgID.GetPubKey())
+	if err != nil {
+		return fmt.Errorf("could not get validator topics: %w", err)
+	}
 	for _, topic := range topics {
 		if err := n.topicsCtrl.Broadcast(topic, encodedMsg, n.cfg.RequestTimeout); err != nil {
-			n.interfaceLogger.Debug("could not broadcast msg", fields.PubKey(vpk), zap.Error(err))
+			n.interfaceLogger.Debug("could not broadcast msg", fields.PubKey(msgID.GetPubKey()), zap.Error(err))
 			return fmt.Errorf("could not broadcast msg: %w", err)
 		}
 	}
@@ -81,7 +86,7 @@ func (n *p2pNetwork) SubscribeAll(logger *zap.Logger) error {
 	if !n.isReady() {
 		return p2pprotocol.ErrNetworkIsNotReady
 	}
-	n.subnets, _ = records.Subnets{}.FromString(records.AllSubnets)
+	n.fixedSubnets, _ = records.Subnets{}.FromString(records.AllSubnets)
 	for subnet := 0; subnet < commons.Subnets(); subnet++ {
 		err := n.topicsCtrl.Subscribe(logger, commons.SubnetTopicID(subnet))
 		if err != nil {
@@ -113,11 +118,11 @@ func (n *p2pNetwork) SubscribeRandoms(logger *zap.Logger, numSubnets int) error
 
 	// Update the subnets slice.
 	subnets := make([]byte, commons.Subnets())
-	copy(subnets, n.subnets)
+	copy(subnets, n.fixedSubnets)
 	for _, subnet := range randomSubnets {
 		subnets[subnet] = byte(1)
 	}
-	n.subnets = subnets
+	n.fixedSubnets = subnets
 
 	return nil
 }
@@ -127,14 +132,46 @@ func (n *p2pNetwork) Subscribe(pk spectypes.ValidatorPK) error {
 	if !n.isReady() {
 		return p2pprotocol.ErrNetworkIsNotReady
 	}
+	share := n.nodeStorage.Shares().Get(nil, pk)
+	if share == nil {
+		return fmt.Errorf("could not find share for validator %s", hex.EncodeToString(pk))
+	}
+	err := n.subscribeCommittee(share.CommitteeID())
+	if err != nil {
+		return fmt.Errorf("could not subscribe to committee: %w", err)
+	}
+	if n.validatorSubnetSubscriptions() {
+		return n.subscribeValidator(pk)
+	}
+	return nil
+}
+
+// subscribeCommittee handles the subscription logic for committee subnets
+func (n *p2pNetwork) subscribeCommittee(cid types.CommitteeID) error {
+	status, found := n.activeCommittees.GetOrInsert(string(cid[:]), validatorStatusSubscribing)
+	if found && status != validatorStatusInactive {
+		return nil
+	}
+	for _, topic := range commons.CommitteeTopicID(cid) {
+		if err := n.topicsCtrl.Subscribe(n.interfaceLogger, topic); err != nil {
+			return fmt.Errorf("could not subscribe to topic %s: %w", topic, err)
+		}
+	}
+	n.activeCommittees.Set(string(cid[:]), validatorStatusSubscribed)
+	return nil
+}
+
+// subscribeValidator handles the subscription logic for validator subnets
+func (n *p2pNetwork) subscribeValidator(pk spectypes.ValidatorPK) error {
 	pkHex := hex.EncodeToString(pk)
 	status, found := n.activeValidators.GetOrInsert(pkHex, validatorStatusSubscribing)
 	if found && status != validatorStatusInactive {
 		return nil
 	}
-	err := n.subscribe(n.interfaceLogger, pk)
-	if err != nil {
-		return err
+	for _, topic := range commons.ValidatorTopicID(pk) {
+		if err := n.topicsCtrl.Subscribe(n.interfaceLogger, topic); err != nil {
+			return fmt.Errorf("could not subscribe to topic %s: %w", topic, err)
+		}
 	}
 	n.activeValidators.Set(pkHex, validatorStatusSubscribed)
 	return nil
@@ -142,31 +179,35 @@ func (n *p2pNetwork) Subscribe(pk spectypes.ValidatorPK) error {
 
 // Unsubscribe unsubscribes from the validator subnet
 func (n *p2pNetwork) Unsubscribe(logger *zap.Logger, pk spectypes.ValidatorPK) error {
+	// TODO: this is unsused, but should probably be implemented.
+	return errors.New("not implemented")
+
+	// if !n.isReady() {
+	// 	return p2pprotocol.ErrNetworkIsNotReady
+	// }
+	// pkHex := hex.EncodeToString(pk)
+	// if status, _ := n.activeValidators.Get(pkHex); status != validatorStatusSubscribed {
+	// 	return nil
+	// }
+	// topics := commons.ValidatorTopicID(pk)
+	// for _, topic := range topics {
+	// 	if err := n.topicsCtrl.Unsubscribe(logger, topic, false); err != nil {
+	// 		return err
+	// 	}
+	// }
+	// n.activeValidators.Del(pkHex)
+	// return nil
+}
+
+func (n *p2pNetwork) unsubscribeSubnet(logger *zap.Logger, subnet uint) error {
 	if !n.isReady() {
 		return p2pprotocol.ErrNetworkIsNotReady
 	}
-	pkHex := hex.EncodeToString(pk)
-	if status, _ := n.activeValidators.Get(pkHex); status != validatorStatusSubscribed {
-		return nil
+	if subnet >= uint(commons.Subnets()) {
+		return fmt.Errorf("invalid subnet %d", subnet)
 	}
-	topics := commons.ValidatorTopicID(pk)
-	for _, topic := range topics {
-		if err := n.topicsCtrl.Unsubscribe(logger, topic, false); err != nil {
-			return err
-		}
-	}
-	n.activeValidators.Del(pkHex)
-	return nil
-}
-
-// subscribe to validator topics, as defined in the fork
-func (n *p2pNetwork) subscribe(logger *zap.Logger, pk spectypes.ValidatorPK) error {
-	topics := commons.ValidatorTopicID(pk)
-	for _, topic := range topics {
-		if err := n.topicsCtrl.Subscribe(logger, topic); err != nil {
-			// return errors.Wrap(err, "could not broadcast message")
-			return err
-		}
+	if err := n.topicsCtrl.Unsubscribe(logger, commons.SubnetTopicID(int(subnet)), false); err != nil {
+		return fmt.Errorf("could not unsubscribe from subnet %d: %w", subnet, err)
 	}
 	return nil
 }
@@ -193,13 +234,6 @@ func (n *p2pNetwork) handlePubsubMessages(logger *zap.Logger) func(ctx context.C
 			return errors.New("message was not decoded")
 		}
 
-		//p2pID := decodedMsg.GetID().String()
-
-		//	logger.With(
-		// 		zap.String("pubKey", hex.EncodeToString(ssvMsg.MsgID.GetPubKey())),
-		// 		zap.String("role", ssvMsg.MsgID.GetRoleType().String()),
-		// 	).Debug("handlePubsubMessages")
-
 		metricsRouterIncoming.WithLabelValues(message.MsgTypeToString(decodedMsg.MsgType)).Inc()
 
 		n.msgRouter.Route(ctx, decodedMsg)
@@ -210,11 +244,11 @@ func (n *p2pNetwork) handlePubsubMessages(logger *zap.Logger) func(ctx context.C
 
 // subscribeToSubnets subscribes to all the node's subnets
 func (n *p2pNetwork) subscribeToSubnets(logger *zap.Logger) error {
-	if len(n.subnets) == 0 {
+	if len(n.fixedSubnets) == 0 {
 		return nil
 	}
-	logger.Debug("subscribing to subnets", fields.Subnets(n.subnets))
-	for i, val := range n.subnets {
+	logger.Debug("subscribing to fixed subnets", fields.Subnets(n.fixedSubnets))
+	for i, val := range n.fixedSubnets {
 		if val > 0 {
 			subnet := fmt.Sprintf("%d", i)
 			if err := n.topicsCtrl.Subscribe(logger, subnet); err != nil {
@@ -226,3 +260,15 @@ func (n *p2pNetwork) subscribeToSubnets(logger *zap.Logger) error {
 	}
 	return nil
 }
+
+func (n *p2pNetwork) validatorTopics(pk spectypes.ValidatorPK) ([]string, error) {
+	if n.cfg.Network.CommitteeSubnetsFork() {
+		share := n.nodeStorage.Shares().Get(nil, pk)
+		if share == nil {
+			return nil, fmt.Errorf("could not find share for validator %s", hex.EncodeToString(pk))
+		}
+		cid := share.CommitteeID()
+		return commons.CommitteeTopicID(cid), nil
+	}
+	return commons.ValidatorTopicID(pk), nil
+}
diff --git a/network/p2p/p2p_setup.go b/network/p2p/p2p_setup.go
index 47ba757b1..11a047794 100644
--- a/network/p2p/p2p_setup.go
+++ b/network/p2p/p2p_setup.go
@@ -94,9 +94,9 @@ func (n *p2pNetwork) initCfg() error {
 		if err != nil {
 			return fmt.Errorf("parse subnet: %w", err)
 		}
-		n.subnets = subnets
+		n.fixedSubnets = subnets
 	} else {
-		n.subnets = make(records.Subnets, p2pcommons.Subnets())
+		n.fixedSubnets = make(records.Subnets, p2pcommons.Subnets())
 	}
 	if n.cfg.MaxPeers <= 0 {
 		n.cfg.MaxPeers = minPeersBuffer
@@ -174,7 +174,7 @@ func (n *p2pNetwork) setupPeerServices(logger *zap.Logger) error {
 	self := records.NewNodeInfo(domain)
 	self.Metadata = &records.NodeMetadata{
 		NodeVersion: commons.GetNodeVersion(),
-		Subnets:     records.Subnets(n.subnets).String(),
+		Subnets:     records.Subnets(n.fixedSubnets).String(),
 	}
 	getPrivKey := func() crypto.PrivKey {
 		return libPrivKey
@@ -195,7 +195,7 @@ func (n *p2pNetwork) setupPeerServices(logger *zap.Logger) error {
 	}
 
 	subnetsProvider := func() records.Subnets {
-		return n.subnets
+		return n.activeSubnets
 	}
 
 	filters := func() []connections.HandshakeFilter {
@@ -241,8 +241,9 @@ func (n *p2pNetwork) setupDiscovery(logger *zap.Logger) error {
 			Bootnodes:     n.cfg.TransformBootnodes(),
 			EnableLogging: n.cfg.DiscoveryTrace,
 		}
-		if len(n.subnets) > 0 {
-			discV5Opts.Subnets = n.subnets
+		if len(n.fixedSubnets) > 0 {
+			discV5Opts.Subnets = n.fixedSubnets
+			logger = logger.With(zap.String("subnets", records.Subnets(n.fixedSubnets).String()))
 		}
 		logger.Info("discovery: using discv5", zap.Strings("bootnodes", discV5Opts.Bootnodes))
 	} else {
diff --git a/network/p2p/p2p_sync.go b/network/p2p/p2p_sync.go
index 40c28c264..28c35719f 100644
--- a/network/p2p/p2p_sync.go
+++ b/network/p2p/p2p_sync.go
@@ -85,7 +85,10 @@ func (n *p2pNetwork) handleStream(logger *zap.Logger, handler p2pprotocol.Reques
 func (n *p2pNetwork) getSubsetOfPeers(logger *zap.Logger, vpk spectypes.ValidatorPK, maxPeers int, filter func(peer.ID) bool) (peers []peer.ID, err error) {
 	var ps []peer.ID
 	seen := make(map[peer.ID]struct{})
-	topics := commons.ValidatorTopicID(vpk)
+	topics, err := n.validatorTopics(vpk)
+	if err != nil {
+		return nil, fmt.Errorf("could not get validator topics: %w", err)
+	}
 	for _, topic := range topics {
 		ps, err = n.topicsCtrl.Peers(topic)
 		if err != nil {
diff --git a/network/p2p/test_utils.go b/network/p2p/test_utils.go
index f6402af29..8ed08c16c 100644
--- a/network/p2p/test_utils.go
+++ b/network/p2p/test_utils.go
@@ -152,8 +152,14 @@ func (ln *LocalNet) NewTestP2pNetwork(ctx context.Context, nodeIndex int, keys t
 	cfg.MessageValidator = validation.NewMessageValidator(networkconfig.TestNetwork)
 	cfg.Network = networkconfig.TestNetwork
 	if options.TotalValidators > 0 {
-		cfg.GetValidatorStats = func() (uint64, uint64, uint64, error) {
-			return uint64(options.TotalValidators), uint64(options.ActiveValidators), uint64(options.MyValidators), nil
+		cfg.GetValidatorStats = func() (network.ValidatorStats, error) {
+			// return uint64(options.TotalValidators), uint64(options.ActiveValidators), uint64(options.MyValidators), nil
+			counts := network.ValidatorCounts{Total: uint32(options.TotalValidators), Attesting: uint32(options.ActiveValidators), Mine: uint32(options.MyValidators)}
+			return network.ValidatorStats{
+					ValidatorCounts: counts,
+					Subnets:         [commons.SubnetsCount]network.ValidatorCounts{1: counts},
+				},
+				nil
 		}
 	}
 
diff --git a/network/topics/container.go b/network/topics/container.go
index a810946f8..4fd2a3244 100644
--- a/network/topics/container.go
+++ b/network/topics/container.go
@@ -67,16 +67,17 @@ func (tc *topicsContainer) Join(name string, opts ...pubsub.TopicOpt) (*pubsub.T
 	return topic, nil
 }
 
-func (tc *topicsContainer) Unsubscribe(name string) {
+func (tc *topicsContainer) Unsubscribe(name string) bool {
 	tc.subLock.Lock()
 	defer tc.subLock.Unlock()
 
 	sub, ok := tc.subs[name]
 	if !ok {
-		return
+		return false
 	}
 	delete(tc.subs, name)
 	sub.Cancel()
+	return true
 }
 
 func (tc *topicsContainer) Subscribe(name string, opts ...pubsub.SubOpt) (*pubsub.Subscription, error) {
diff --git a/network/topics/controller.go b/network/topics/controller.go
index 2a2489570..10217d0f0 100644
--- a/network/topics/controller.go
+++ b/network/topics/controller.go
@@ -2,13 +2,14 @@ package topics
 
 import (
 	"context"
+	"errors"
+	"fmt"
 	"io"
 	"strconv"
 	"time"
 
 	pubsub "github.com/libp2p/go-libp2p-pubsub"
 	"github.com/libp2p/go-libp2p/core/peer"
-	"github.com/pkg/errors"
 	"go.uber.org/zap"
 
 	"github.com/ssvlabs/ssv/network/commons"
@@ -32,6 +33,8 @@ type Controller interface {
 	Topics() []string
 	// Broadcast publishes the message on the given topic
 	Broadcast(topicName string, data []byte, timeout time.Duration) error
+	// UpdateScoreParams refreshes the score params for every subscribed topic.
+	UpdateScoreParams(logger *zap.Logger) error
 
 	io.Closer
 }
@@ -104,6 +107,31 @@ func (ctrl *topicsCtrl) onNewTopic(logger *zap.Logger) onTopicJoined {
 	}
 }
 
+func (ctrl *topicsCtrl) UpdateScoreParams(logger *zap.Logger) error {
+	if ctrl.scoreParamsFactory == nil {
+		return fmt.Errorf("scoreParamsFactory is not set")
+	}
+	var err error
+	topics := ctrl.ps.GetTopics()
+	for _, topicName := range topics {
+		topic := ctrl.container.Get(topicName)
+		if topic == nil {
+			err = errors.Join(err, fmt.Errorf("topic %s is not ready", topicName))
+			continue
+		}
+		p := ctrl.scoreParamsFactory(topicName)
+		if p == nil {
+			err = errors.Join(err, fmt.Errorf("score params for topic %s is nil", topicName))
+			continue
+		}
+		if err := topic.SetScoreParams(p); err != nil {
+			err = errors.Join(err, fmt.Errorf("could not set score params for topic %s: %w", topicName, err))
+			continue
+		}
+	}
+	return err
+}
+
 // Close implements io.Closer
 func (ctrl *topicsCtrl) Close() error {
 	topics := ctrl.ps.GetTopics()
@@ -179,7 +207,10 @@ func (ctrl *topicsCtrl) Broadcast(name string, data []byte, timeout time.Duratio
 // Unsubscribe unsubscribes from the given topic, only if there are no other subscribers of the given topic
 // if hard is true, we will unsubscribe the topic even if there are more subscribers.
 func (ctrl *topicsCtrl) Unsubscribe(logger *zap.Logger, name string, hard bool) error {
-	ctrl.container.Unsubscribe(name)
+	name = commons.GetTopicFullName(name)
+	if !ctrl.container.Unsubscribe(name) {
+		return fmt.Errorf("failed to unsubscribe from topic %s: not subscribed", name)
+	}
 
 	if ctrl.msgValidator != nil {
 		err := ctrl.ps.UnregisterTopicValidator(name)
@@ -258,15 +289,18 @@ func (ctrl *topicsCtrl) listen(logger *zap.Logger, sub *pubsub.Subscription) err
 func (ctrl *topicsCtrl) setupTopicValidator(name string) error {
 	if ctrl.msgValidator != nil {
 		// first try to unregister in case there is already a msg validator for that topic (e.g. fork scenario)
-		_ = ctrl.ps.UnregisterTopicValidator(name)
+		err := ctrl.ps.UnregisterTopicValidator(name)
+		if err != nil {
+			ctrl.logger.Debug("failed to unregister topic validator", zap.String("topic", name), zap.Error(err))
+		}
 
 		var opts []pubsub.ValidatorOpt
 		// Optional: set a timeout for message validation
 		// opts = append(opts, pubsub.WithValidatorTimeout(time.Second))
 
-		err := ctrl.ps.RegisterTopicValidator(name, ctrl.msgValidator.ValidatorForTopic(name), opts...)
+		err = ctrl.ps.RegisterTopicValidator(name, ctrl.msgValidator.ValidatorForTopic(name), opts...)
 		if err != nil {
-			return errors.Wrap(err, "could not register topic validator")
+			return fmt.Errorf("could not register topic validator: %w", err)
 		}
 	}
 	return nil
diff --git a/network/topics/params/topic_score.go b/network/topics/params/topic_score.go
index 6bab61eac..3e386e410 100644
--- a/network/topics/params/topic_score.go
+++ b/network/topics/params/topic_score.go
@@ -11,7 +11,7 @@ import (
 const (
 	// Network Topology
 	gossipSubD          = 8
-	minActiveValidators = 200
+	minActiveValidators = 1
 
 	// Overall parameters
 	totalTopicsWeight = 4.0
@@ -45,8 +45,6 @@ var (
 
 // NetworkOpts is the config struct for network configurations
 type NetworkOpts struct {
-	// ActiveValidators is the amount of validators in the network
-	ActiveValidators int
 	// Subnets is the number of subnets in the network
 	Subnets int
 	// OneEpochDuration is used as a time-frame length to control scoring in a dynamic way
@@ -57,6 +55,9 @@ type NetworkOpts struct {
 
 // TopicOpts is the config struct for topic configurations
 type TopicOpts struct {
+	// ActiveValidators is the amount of validators in the topic
+	ActiveValidators int
+
 	// D is the gossip degree
 	D int
 
@@ -144,7 +145,7 @@ func (o *Options) defaults() {
 }
 
 func (o *Options) validate() error {
-	if o.Network.ActiveValidators < minActiveValidators {
+	if o.Topic.ActiveValidators < minActiveValidators {
 		return ErrLowValidatorsCount
 	}
 	return nil
@@ -157,18 +158,23 @@ func (o *Options) maxScore() float64 {
 
 // NewOpts creates new TopicOpts instance
 func NewOpts(activeValidators, subnets int) Options {
+	if activeValidators < minActiveValidators {
+		// TODO: is this right? For subnets that don't have validators, should we even create a topic?
+		// Currently this is required because we sometimes update score params before unsubscribing from old topics.
+		activeValidators = minActiveValidators
+	}
 	return Options{
 		Network: NetworkOpts{
+			Subnets: subnets,
+		},
+		Topic: TopicOpts{
 			ActiveValidators: activeValidators,
-			Subnets:          subnets,
 		},
-		Topic: TopicOpts{},
 	}
 }
 
 // NewSubnetTopicOpts creates new TopicOpts for a subnet topic
 func NewSubnetTopicOpts(activeValidators, subnets int) Options {
-
 	// Create options with default values
 	opts := NewOpts(activeValidators, subnets)
 	opts.defaults()
@@ -177,9 +183,8 @@ func NewSubnetTopicOpts(activeValidators, subnets int) Options {
 	opts.Topic.TopicWeight = opts.Network.TotalTopicsWeight / float64(opts.Network.Subnets)
 
 	// Set expected message rate based on stage metrics
-	validatorsPerSubnet := float64(opts.Network.ActiveValidators) / float64(opts.Network.Subnets)
 	msgsPerValidatorPerSecond := 600.0 / 10000.0
-	opts.Topic.ExpectedMsgRate = validatorsPerSubnet * msgsPerValidatorPerSecond
+	opts.Topic.ExpectedMsgRate = float64(opts.Topic.ActiveValidators) * msgsPerValidatorPerSecond
 
 	return opts
 }
diff --git a/network/topics/pubsub.go b/network/topics/pubsub.go
index 8ef1ab725..60976723e 100644
--- a/network/topics/pubsub.go
+++ b/network/topics/pubsub.go
@@ -159,9 +159,13 @@ func NewPubSub(ctx context.Context, logger *zap.Logger, cfg *PubSubConfig, metri
 		psOpts = append(psOpts, pubsub.WithPeerScore(peerScoreParams, params.PeerScoreThresholds()),
 			pubsub.WithPeerScoreInspect(inspector, inspectInterval))
 		if cfg.GetValidatorStats == nil {
-			cfg.GetValidatorStats = func() (uint64, uint64, uint64, error) {
+			cfg.GetValidatorStats = func() (network.ValidatorStats, error) {
 				// default in case it was not injected
-				return 100, 100, 10, nil
+				counts := network.ValidatorCounts{Total: 100, Attesting: 100, Mine: 10}
+				return network.ValidatorStats{
+					ValidatorCounts: counts,
+					Subnets:         [commons.SubnetsCount]network.ValidatorCounts{counts},
+				}, nil
 			}
 		}
 		topicScoreFactory = topicScoreParams(logger, cfg)
diff --git a/network/topics/scoring.go b/network/topics/scoring.go
index a9be3d569..a92bf0f27 100644
--- a/network/topics/scoring.go
+++ b/network/topics/scoring.go
@@ -2,6 +2,7 @@ package topics
 
 import (
 	"math"
+	"strconv"
 	"time"
 
 	"github.com/ssvlabs/ssv/logging/fields"
@@ -97,15 +98,26 @@ func scoreInspector(logger *zap.Logger, scoreIdx peers.ScoreIndex, logFrequency
 // topicScoreParams factory for creating scoring params for topics
 func topicScoreParams(logger *zap.Logger, cfg *PubSubConfig) func(string) *pubsub.TopicScoreParams {
 	return func(t string) *pubsub.TopicScoreParams {
-		totalValidators, activeValidators, myValidators, err := cfg.GetValidatorStats()
+		validatorStats, err := cfg.GetValidatorStats()
 		if err != nil {
 			logger.Debug("could not read stats: active validators")
 			return nil
 		}
-		logger := logger.With(zap.String("topic", t), zap.Uint64("totalValidators", totalValidators),
-			zap.Uint64("activeValidators", activeValidators), zap.Uint64("myValidators", myValidators))
+		subnet, err := strconv.ParseUint(commons.GetTopicBaseName(t), 10, 64)
+		if err != nil {
+			// TODO: this would fail if we add topics that are not subnets.
+			logger.Error("could not parse subnet from topic", zap.String("topic", t))
+			return nil
+		}
+		stats := validatorStats.Subnets[subnet]
+		logger := logger.With(
+			zap.String("topic", t),
+			zap.Uint64("total_validators", uint64(stats.Total)),
+			zap.Uint64("attesting_validators", uint64(stats.Attesting)),
+			zap.Uint64("my_validators", uint64(stats.Mine)))
 		logger.Debug("got validator stats for score params")
-		opts := params.NewSubnetTopicOpts(int(totalValidators), commons.Subnets())
+		// TODO: we use total instead of attesting here, is that correct?
+		opts := params.NewSubnetTopicOpts(int(stats.Total), commons.Subnets())
 		tp, err := params.TopicParams(opts)
 		if err != nil {
 			logger.Debug("ignoring topic score params", zap.Error(err))
diff --git a/networkconfig/config.go b/networkconfig/config.go
index 3485c8bbf..0acef1867 100644
--- a/networkconfig/config.go
+++ b/networkconfig/config.go
@@ -38,6 +38,8 @@ type NetworkConfig struct {
 	RegistrySyncOffset   *big.Int
 	RegistryContractAddr string // TODO: ethcommon.Address
 	Bootnodes            []string
+
+	CommitteeSubnetForkEpoch spec.Epoch
 }
 
 func (n NetworkConfig) String() string {
@@ -68,3 +70,7 @@ func (n NetworkConfig) SlotsPerEpoch() uint64 {
 func (n NetworkConfig) GetGenesisTime() time.Time {
 	return time.Unix(int64(n.Beacon.MinGenesisTime()), 0)
 }
+
+func (n NetworkConfig) CommitteeSubnetsFork() bool {
+	return n.Beacon.EstimatedCurrentEpoch() >= n.CommitteeSubnetForkEpoch
+}
diff --git a/networkconfig/holesky-stage.go b/networkconfig/holesky-stage.go
index 2075a9646..d8a4294d1 100644
--- a/networkconfig/holesky-stage.go
+++ b/networkconfig/holesky-stage.go
@@ -11,11 +11,12 @@ import (
 var HoleskyStage = NetworkConfig{
 	Name:                 "holesky-stage",
 	Beacon:               beacon.NewNetwork(spectypes.HoleskyNetwork),
-	Domain:               [4]byte{0x00, 0x00, 0x31, 0x12},
+	Domain:               [4]byte{0x00, 0x00, 0x31, 0x98},
 	GenesisEpoch:         1,
 	RegistrySyncOffset:   new(big.Int).SetInt64(84599),
 	RegistryContractAddr: "0x0d33801785340072C452b994496B19f196b7eE15",
 	Bootnodes: []string{
 		"enr:-Li4QPnPGESWx2wnu3s2qeu6keFbkaV2M0ZiGHgxxGI9ThP4XSgSaFzl6zYsF1zAdni3Mh04iA6BEZqoC6LZ52UFnwKGAYxEgLqeh2F0dG5ldHOIAAAAAAAAAACEZXRoMpD1pf1CAAAAAP__________gmlkgnY0gmlwhDQiKqmJc2VjcDI1NmsxoQP2e508AoA0B-KH-IaAd3nVCfI9q16lNztV-oTpcH72tIN0Y3CCE4mDdWRwgg-h",
 	},
+	CommitteeSubnetForkEpoch: 54663,
 }
diff --git a/operator/validator/controller.go b/operator/validator/controller.go
index 88fe2684d..f53ee2019 100644
--- a/operator/validator/controller.go
+++ b/operator/validator/controller.go
@@ -25,6 +25,8 @@ import (
 	"github.com/ssvlabs/ssv/logging/fields"
 	"github.com/ssvlabs/ssv/message/validation"
 	"github.com/ssvlabs/ssv/network"
+	"github.com/ssvlabs/ssv/network/commons"
+	"github.com/ssvlabs/ssv/networkconfig"
 	operatordatastore "github.com/ssvlabs/ssv/operator/datastore"
 	"github.com/ssvlabs/ssv/operator/duties"
 	"github.com/ssvlabs/ssv/operator/keys"
@@ -65,7 +67,7 @@ type ControllerOptions struct {
 	MetadataUpdateInterval     time.Duration `yaml:"MetadataUpdateInterval" env:"METADATA_UPDATE_INTERVAL" env-default:"12m" env-description:"Interval for updating metadata"`
 	HistorySyncBatchSize       int           `yaml:"HistorySyncBatchSize" env:"HISTORY_SYNC_BATCH_SIZE" env-default:"25" env-description:"Maximum number of messages to sync in a single batch"`
 	MinPeers                   int           `yaml:"MinimumPeers" env:"MINIMUM_PEERS" env-default:"2" env-description:"The required minimum peers for sync"`
-	BeaconNetwork              beaconprotocol.Network
+	NetworkConfig              networkconfig.NetworkConfig
 	Network                    P2PNetwork
 	Beacon                     beaconprotocol.BeaconNode
 	FullNode                   bool `yaml:"FullNode" env:"FULLNODE" env-default:"false" env-description:"Save decided history rather than just highest messages"`
@@ -103,7 +105,7 @@ type Controller interface {
 	//  - the amount of validators in the network
 	//  - the amount of active validators (i.e. not slashed or existed)
 	//  - the amount of validators assigned to this operator
-	GetValidatorStats() (uint64, uint64, uint64, error)
+	GetValidatorStats() (network.ValidatorStats, error)
 	IndicesChangeChan() chan struct{}
 	ValidatorExitChan() <-chan duties.ExitDescriptor
 
@@ -152,6 +154,7 @@ type controller struct {
 	recipientsStorage Recipients
 	ibftStorageMap    *storage.QBFTStores
 
+	networkCfg     networkconfig.NetworkConfig
 	beacon         beaconprotocol.BeaconNode
 	keyManager     spectypes.KeyManager
 	operatorSigner spectypes.OperatorSigner
@@ -180,6 +183,11 @@ type controller struct {
 	metadataLastUpdated       map[string]time.Time
 	indicesChange             chan struct{}
 	validatorExitCh           chan duties.ExitDescriptor
+
+	validatorStats         *network.ValidatorStats
+	validatorStatsEpoch    phase0.Epoch
+	validatorStatsMutex    sync.Mutex
+	validatorStatsPostFork bool
 }
 
 // NewController creates a new validator controller instance
@@ -200,7 +208,7 @@ func NewController(logger *zap.Logger, options ControllerOptions) Controller {
 	validatorOptions := validator.Options{ //TODO add vars
 		Network:       options.Network,
 		Beacon:        options.Beacon,
-		BeaconNetwork: options.BeaconNetwork.GetNetwork(),
+		BeaconNetwork: options.NetworkConfig.Beacon.GetNetwork(),
 		Storage:       options.StorageMap,
 		//Share:   nil,  // set per validator
 		Signer:            options.KeyManager,
@@ -238,6 +246,7 @@ func NewController(logger *zap.Logger, options ControllerOptions) Controller {
 		recipientsStorage: options.RegistryStorage,
 		ibftStorageMap:    options.StorageMap,
 		context:           options.Context,
+		networkCfg:        options.NetworkConfig,
 		beacon:            options.Beacon,
 		operatorDataStore: options.OperatorDataStore,
 		keyManager:        options.KeyManager,
@@ -300,19 +309,52 @@ func (c *controller) ValidatorExitChan() <-chan duties.ExitDescriptor {
 	return c.validatorExitCh
 }
 
-func (c *controller) GetValidatorStats() (uint64, uint64, uint64, error) {
-	allShares := c.sharesStorage.List(nil)
-	operatorShares := uint64(0)
-	active := uint64(0)
-	for _, s := range allShares {
+func (c *controller) GetValidatorStats() (network.ValidatorStats, error) {
+	c.validatorStatsMutex.Lock()
+	defer c.validatorStatsMutex.Unlock()
+
+	postFork := c.networkCfg.CommitteeSubnetsFork()
+
+	if c.validatorStats != nil &&
+		c.validatorStatsEpoch == c.beacon.GetBeaconNetwork().EstimatedCurrentEpoch() &&
+		c.validatorStatsPostFork == postFork {
+		return *c.validatorStats, nil
+	}
+
+	start := time.Now()
+	all := network.ValidatorStats{}
+	for _, s := range c.sharesStorage.List(nil) {
+		var subnetNumber int
+		if postFork {
+			subnetNumber = commons.CommitteeSubnet(s.CommitteeID())
+		} else {
+			subnetNumber = commons.ValidatorSubnet(hex.EncodeToString(s.ValidatorPubKey))
+		}
+		subnet := all.Subnets[subnetNumber]
+
 		if ok := s.BelongsToOperator(c.operatorDataStore.GetOperatorID()); ok {
-			operatorShares++
+			all.Mine++
+			subnet.Mine++
 		}
 		if s.IsAttesting(c.beacon.GetBeaconNetwork().EstimatedCurrentEpoch()) {
-			active++
+			all.Attesting++
+			subnet.Attesting++
 		}
-	}
-	return uint64(len(allShares)), active, operatorShares, nil
+		all.Total++
+		subnet.Total++
+		all.Subnets[subnetNumber] = subnet
+	}
+
+	c.validatorStats = &all
+	c.validatorStatsEpoch = c.beacon.GetBeaconNetwork().EstimatedCurrentEpoch()
+	c.validatorStatsPostFork = postFork
+	c.logger.Debug(
+		"computed validator stats",
+		zap.Any("stats", all),
+		fields.Took(time.Since(start)),
+		zap.Bool("post_fork", postFork),
+	)
+	return all, nil
 }
 
 func (c *controller) handleRouterMessages() {
diff --git a/protocol/v2/types/ssvshare.go b/protocol/v2/types/ssvshare.go
index 8cde213e7..5117b80a3 100644
--- a/protocol/v2/types/ssvshare.go
+++ b/protocol/v2/types/ssvshare.go
@@ -1,6 +1,7 @@
 package types
 
 import (
+	"crypto/sha256"
 	"encoding/binary"
 	"sort"
 
@@ -19,10 +20,13 @@ const (
 	MaxAllowedShareSize  = MaxPossibleShareSize * 8 // Leaving some room for protocol updates and calculation mistakes.
 )
 
+type CommitteeID [32]byte
+
 // SSVShare is a combination of spectypes.Share and its Metadata.
 type SSVShare struct {
 	spectypes.Share
 	Metadata
+	committeeID *CommitteeID
 }
 
 // BelongsToOperator checks whether the share belongs to operator.
@@ -44,6 +48,19 @@ func (s *SSVShare) SetFeeRecipient(feeRecipient bellatrix.ExecutionAddress) {
 	s.FeeRecipientAddress = feeRecipient
 }
 
+func (s *SSVShare) CommitteeID() CommitteeID {
+	if s.committeeID != nil {
+		return *s.committeeID
+	}
+	ids := make([]spectypes.OperatorID, len(s.Share.Committee))
+	for i, v := range s.Share.Committee {
+		ids[i] = v.OperatorID
+	}
+	id := ComputeCommitteeID(ids)
+	s.committeeID = &id
+	return id
+}
+
 // ComputeClusterIDHash will compute cluster ID hash with given owner address and operator ids
 func ComputeClusterIDHash(address common.Address, operatorIds []uint64) []byte {
 	sort.Slice(operatorIds, func(i, j int) bool {
@@ -80,3 +97,18 @@ type Metadata struct {
 	OwnerAddress   common.Address
 	Liquidated     bool
 }
+
+// Return a 32 bytes ID for the committee of operators
+func ComputeCommitteeID(committee []spectypes.OperatorID) CommitteeID {
+	// sort
+	sort.Slice(committee, func(i, j int) bool {
+		return committee[i] < committee[j]
+	})
+	// Convert to bytes
+	bytes := make([]byte, len(committee)*4)
+	for i, v := range committee {
+		binary.LittleEndian.PutUint32(bytes[i*4:], uint32(v))
+	}
+	// Hash
+	return CommitteeID(sha256.Sum256(bytes))
+}
diff --git a/registry/storage/shares.go b/registry/storage/shares.go
index 6de89a05c..aef1f87d4 100644
--- a/registry/storage/shares.go
+++ b/registry/storage/shares.go
@@ -263,7 +263,7 @@ func (s *sharesStorage) storageShareToSpecShare(share *storageShare) (*types.SSV
 			Committee:           committee,
 			Quorum:              share.Quorum,
 			PartialQuorum:       share.PartialQuorum,
-			DomainType:          share.DomainType,
+			DomainType:          types.GetDefaultDomain(), // TODO: stop persisting domain types in shares?
 			FeeRecipientAddress: share.FeeRecipientAddress,
 			Graffiti:            share.Graffiti,
 		},
